{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# CONFIG\n",
        "INPUT_FILE = \"/content/drive/MyDrive/eve.json-202506030000\" # Updated input file\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/processed_chunks_by_time_json\" # New output directory for JSON\n",
        "# TIME_CHUNK_SIZE = \"1H\"  # This is now just for clarity, the logic will align with hour boundaries\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def extract_features(log_entry):\n",
        "    # Customize this based on your log structure\n",
        "    column_names = ['timestamp', 'flow_id', 'src_ip', 'src_port',\n",
        "       'dest_ip', 'dest_port', 'proto', 'event_type', 'flow']\n",
        "\n",
        "    features = {}\n",
        "    for col in column_names:\n",
        "      features[col] = log_entry.get(col)\n",
        "    return features\n",
        "\n",
        "def process_large_json_by_time(input_file):\n",
        "    chunk = []\n",
        "    current_chunk_hour = None\n",
        "    chunk_count = 0\n",
        "\n",
        "    with open(input_file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            try:\n",
        "                log_entry = json.loads(line)\n",
        "\n",
        "                timestamp_str = log_entry.get('timestamp')  # Assuming 'timestamp' field exists\n",
        "                event_type = log_entry.get('event_type') # Get the event type\n",
        "\n",
        "                # Parse the timestamp string into a datetime object\n",
        "                timestamp = datetime.datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S.%f%z')\n",
        "\n",
        "                # Determine the hour of the current timestamp\n",
        "                current_hour = timestamp.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "                # Check if a new chunk should be started\n",
        "                if current_chunk_hour is None:\n",
        "                    current_chunk_hour = current_hour\n",
        "                elif current_hour > current_chunk_hour:\n",
        "                    # Save the current chunk\n",
        "                    output_filename = f\"{OUTPUT_DIR}/chunk_{chunk_count}_{current_chunk_hour.strftime('%Y%m%d%H%M%S')}.json\"\n",
        "                    with open(output_filename, 'w') as outfile:\n",
        "                        json.dump(chunk, outfile, indent=2) # Use indent for readability\n",
        "                    print(f\"Saved {output_filename}\")\n",
        "\n",
        "                    # Start a new chunk\n",
        "                    chunk = []\n",
        "                    chunk_count += 1\n",
        "                    current_chunk_hour = current_hour\n",
        "\n",
        "                # selected_event_types = ['alert', 'flow', 'anomaly']\n",
        "\n",
        "                # Add the log entry to the current chunk only if the event type is selected\n",
        "                # if event_type in selected_event_types:\n",
        "                features = extract_features(log_entry)\n",
        "                chunk.append(features)\n",
        "\n",
        "            except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
        "                print(f\"Error processing line {i+1}: {e}\")\n",
        "                continue  # Skip malformed or missing timestamp lines\n",
        "\n",
        "        # Save any remaining logs as JSON\n",
        "        if chunk:\n",
        "            output_filename = f\"{OUTPUT_DIR}/chunk_{chunk_count}_{current_chunk_hour.strftime('%Y%m%d%H%M%S')}.json\"\n",
        "            with open(output_filename, 'w') as outfile:\n",
        "                json.dump(chunk, outfile, indent=2)\n",
        "            print(f\"Saved final {output_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Pass only the input file to the function\n",
        "    process_large_json_by_time(INPUT_FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "collapsed": true,
        "id": "74k499kaVGTO",
        "outputId": "02e08c65-37b2-4583-be78-75acbdff6ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_0_20250601160000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_1_20250601170000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_2_20250601180000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_3_20250601190000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_4_20250601200000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_5_20250601210000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_6_20250601220000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_7_20250601230000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_8_20250602000000.json\n",
            "Error processing line 118635984: Expecting ':' delimiter: line 1 column 97 (char 96)\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_9_20250602010000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_10_20250602020000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_11_20250602030000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_12_20250602040000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_13_20250602050000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_14_20250602060000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_15_20250602070000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_16_20250602080000.json\n",
            "Saved /content/drive/MyDrive/processed_chunks_by_time_json/chunk_17_20250602090000.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2796146377.py\u001b[0m in \u001b[0;36mprocess_large_json_by_time\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use indent for readability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved {output_filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2796146377.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Pass only the input file to the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mprocess_large_json_by_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2-2796146377.py\u001b[0m in \u001b[0;36mprocess_large_json_by_time\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;31m# Save the current chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0moutput_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{OUTPUT_DIR}/chunk_{chunk_count}_{current_chunk_hour.strftime('%Y%m%d%H%M%S')}.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use indent for readability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved {output_filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "shutil.make_archive(\"/content/processed_chunks_by_time_json\", 'zip', \"processed_chunks_by_time_json\")\n",
        "files.download('/content/processed_chunks_by_time_json.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "uUP8sL7rubhO",
        "outputId": "c11f9cee-2b3f-4f9f-f118-021243cab194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c45381ec-eca7-4946-9f62-5df77583141c\", \"processed_chunks_by_time_json.zip\", 3681569176)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}